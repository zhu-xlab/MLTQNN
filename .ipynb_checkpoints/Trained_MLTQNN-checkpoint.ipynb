{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d407335-bf86-47d9-9d11-ef3b7bd2688e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 10:29:58.732672: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib64/openmpi/lib:/usr/local/cuda/lib64:/opt/intel/oneapi/redist/lib:/opt/postgresql/15.2/lib\n",
      "2026-01-29 10:29:58.732695: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/dss/dsshome1/lxc0C/ge32luv/myenv/lib/python3.9/site-packages/')\n",
    "sys.path.reverse()\n",
    "\n",
    "import site\n",
    "import importlib\n",
    "import pkg_resources\n",
    "\n",
    "site.addsitedir('/dss/dsshome1/lxc0C/ge32luv/myenv/lib/python3.9/site-packages/')\n",
    "\n",
    "importlib.reload(pkg_resources)\n",
    "pkg_resources.get_distribution('google-api-core')\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import struct\n",
    "from array import array\n",
    "from os.path  import join\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image, ImageFilter\n",
    "import scipy.io\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from sklearn import manifold\n",
    "import pickle \n",
    "\n",
    "import seaborn as sns\n",
    "import saliency.core as saliency\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import absl.logging\n",
    "\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e125009-2a10-4a8f-9ca0-65f5d9eb7952",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 10:30:05.608861: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2026-01-29 10:30:05.608900: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (qlm): /proc/driver/nvidia/version does not exist\n",
      "2026-01-29 10:30:05.609401: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import model\n",
    "from dataLoader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74bac2a3-5c14-4281-acfe-4a4c9f5d44aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_mltqnn_models = {'sat': {'0.1': 'doublefine/Generalizability_TNNLS/sat_16_QCNN_patch_U3locWeight_weightRatio5.0_234244', \n",
    "                               '0.5': 'doublefine/Generalizability_TNNLS/sat_18_QCNN_patch_U3locWeight_weightRatio5.0_184406', \n",
    "                               '1':'doublefine/Generalizability_TNNLS/sat_17_QCNN_patch_U3locWeight_weightRatio5.0_133305'},\n",
    "                       'lcz': {'0.1': 'doublefine/Generalizability_TNNLS/lcz_22_QCNN_patch_U3locWeight_weightRatio5.0_191037', \n",
    "                               '0.5': 'doublefine/Generalizability_TNNLS/lcz_19_QCNN_patch_U3locWeight_weightRatio5.0_161422', \n",
    "                               '1':'doublefine/Generalizability_TNNLS/lcz_17_QCNN_patch_U3locWeight_weightRatio5.0_190015'},\n",
    "                       'eurosat': {'0.1': 'doublefine/Generalizability_TNNLS/eurosat_16_QCNN_patch_U3locWeight_weightRatio3.0_070843', \n",
    "                                   '0.5': 'doublefine/Generalizability_TNNLS/eurosat_18_QCNN_patch_U3locWeight_weightRatio10.0_174226', \n",
    "                                   '1':'doublefine/Generalizability_TNNLS/eurosat_16_QCNN_patch_U3locWeight_weightRatio5.0_030642'},  \n",
    "                       'patternet': {'0.1': 'doublefine/Generalizability_TNNLS/patternet_16_QCNN_patch_U3locWeight_weightRatio10.0_092139', \n",
    "                                     '0.5': 'doublefine/Generalizability_TNNLS/patternet_17_QCNN_patch_U3locWeight_weightRatio10.0_005504', \n",
    "                                     '1':'doublefine/Generalizability_TNNLS/patternet_16_QCNN_patch_U3locWeight_weightRatio7.0_074300'},\n",
    "                       'location': 'experiments/wandb/'}\n",
    "\n",
    "saved_models = {'qcnn': trained_mltqnn_models}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd99ff3a-2c56-4f0c-90d0-d9b904e4c766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: sat, label_ratio: 0.1\n",
      "38/38 [==============================] - 32s 725ms/step - loss: 0.2269 - classifier_loss: 0.2189 - reconstruction_loss: 0.0080 - classifier_accuracy: 0.9150\n",
      "dataset: sat, label_ratio: 0.5\n",
      "38/38 [==============================] - 31s 687ms/step - loss: 0.1526 - classifier_loss: 0.1451 - reconstruction_loss: 0.0075 - classifier_accuracy: 0.9525\n",
      "dataset: sat, label_ratio: 1\n",
      "38/38 [==============================] - 31s 688ms/step - loss: 0.1109 - classifier_loss: 0.1045 - reconstruction_loss: 0.0064 - classifier_accuracy: 0.9592\n"
     ]
    }
   ],
   "source": [
    "def get_model(saved_models, target_model, dataset, label_ratio):\n",
    "    target_run = saved_models[target_model][dataset][str(label_ratio)]\n",
    "    wandb_path = saved_models[target_model]['location']\n",
    "    qcnn_model, test_acc = model.build_model(target_model, dataset, label_ratio, labeled_train_x.shape[1:], len(class_name), target_run, wandb_path)\n",
    "    test = model.matrices(qcnn_model, test_x, test_y, 'test')\n",
    "    assert test_acc == test[3]\n",
    "    \n",
    "    return qcnn_model\n",
    "\n",
    "label_ratios = [0.1, 0.5, 1]\n",
    "datasets = ['sat', 'lcz', 'eurosat', 'patternet']\n",
    "model_type = 'qcnn'\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    for label_ratio in label_ratios:\n",
    "        print(f'dataset: {dataset}, label_ratio: {label_ratio}')\n",
    "        data = DataLoader(dataset, label_ratio)\n",
    "        unlabeled_train_x, unlabeled_train_y, labeled_train_x, labeled_train_y, valid_x, valid_y, test_x, test_y = data.get_data()\n",
    "        class_name = data.get_categories()\n",
    "        obtained_model = get_model(saved_models, model_type, dataset, label_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ff76cc8-7042-4754-b082-581a21808611",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def vis_samples(imgs):\n",
    "#     fig, axs = plt.subplots(1, len(imgs), layout='constrained')\n",
    "#     for i in range(imgs.shape[0]):\n",
    "#         img = imgs[i]\n",
    "#         sample = img[:, :, :3]\n",
    "#         axs[i].imshow(sample)\n",
    "#         axs[i].axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "# def set_seed(seed):\n",
    "#     random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     tf.random.set_seed(seed)\n",
    "#     tf.experimental.numpy.random.seed(seed)\n",
    "#     os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "#     os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "#     os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "#     print(f\"Random seed set as {seed}\")\n",
    "    \n",
    "    \n",
    "# def get_model(saved_models, target_model, dataset, label_ratio):\n",
    "#     target_run = saved_models[target_model][dataset][str(label_ratio)]\n",
    "#     wandb_path = saved_models[target_model]['location']\n",
    "#     qcnn_model, test_acc = model.build_model(target_model, dataset, label_ratio, labeled_train_x.shape[1:], len(class_name), target_run, wandb_path)\n",
    "#     test = model.matrices(qcnn_model, test_x, test_y, 'test')\n",
    "#     assert test_acc == test[3]\n",
    "    \n",
    "#     y_pred, _  = qcnn_model.predict(test_x)\n",
    "#     y_pred = np.argmax(y_pred, axis=1)\n",
    "#     y_true = np.argmax(test_y, axis=1)\n",
    "#     class_report = classification_report(y_true ,y_pred, target_names=class_name, output_dict=True)\n",
    "#     return qcnn_model, class_report\n",
    "\n",
    "# def get_dense_input(model, test_x):\n",
    "#     encoder = tf.keras.models.Model(inputs=[model.input], outputs=[model.get_layer('pqc').output])\n",
    "#     outputs = encoder.predict(test_x)\n",
    "#     return outputs\n",
    "\n",
    "\n",
    "# def temp_model_build(model):\n",
    "#     inputs = tf.keras.Input(shape=(64,))\n",
    "#     classifier = model.get_layer('classifier')(inputs)\n",
    "#     temp_model = tf.keras.models.Model(inputs=[inputs], outputs=[classifier])\n",
    "\n",
    "#     return temp_model\n",
    "\n",
    "    \n",
    "# def qnn_classifier_function(images, call_model_args=None, expected_keys=None):\n",
    "#     target_class_idx =  call_model_args['class_idx_str']\n",
    "#     images = tf.convert_to_tensor(images)\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         tape.watch(images)\n",
    "#         output_layer = qnn_classifier(images)\n",
    "#         output_layer = output_layer[:,target_class_idx]\n",
    "#         gradients = np.array(tape.gradient(output_layer, images))\n",
    "#         return {saliency.base.INPUT_OUTPUT_GRADIENTS: gradients}\n",
    "\n",
    "# def cnn_classifier_function(images, call_model_args=None, expected_keys=None):\n",
    "#     target_class_idx =  call_model_args['class_idx_str']\n",
    "#     images = tf.convert_to_tensor(images)\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         tape.watch(images)\n",
    "#         output_layer = cnn_classifier(images)\n",
    "#         output_layer = output_layer[:,target_class_idx]\n",
    "#         gradients = np.array(tape.gradient(output_layer, images))\n",
    "#         return {saliency.base.INPUT_OUTPUT_GRADIENTS: gradients}\n",
    "    \n",
    "    \n",
    "\n",
    "# def get_guided_ig(model, model_type, seed):\n",
    "#     guided_ig = saliency.GuidedIG()\n",
    "#     features =  get_dense_input(model, test_x)\n",
    "\n",
    "#     test_y_ig = np.argmax(test_y, axis=1)\n",
    "    \n",
    "#     outputs = []\n",
    "#     for i, image in enumerate(features):\n",
    "#         call_model_args = {'class_idx_str': test_y_ig[i]}\n",
    "#         baseline = tf.random.normal(shape=(64,), mean=0, stddev=5.0, seed=seed)        \n",
    "        \n",
    "#         if model_type == 'qcnn':\n",
    "#             qnn_classifier = temp_model_build(model) \n",
    "            \n",
    "#             def call_model_function(images, call_model_args=None, expected_keys=None):\n",
    "#                 target_class_idx =  call_model_args['class_idx_str']\n",
    "#                 images = tf.convert_to_tensor(images)\n",
    "#                 with tf.GradientTape() as tape:\n",
    "#                     tape.watch(images)\n",
    "#                     output_layer = qnn_classifier(images)\n",
    "#                     output_layer = output_layer[:,target_class_idx]\n",
    "#                     gradients = np.array(tape.gradient(output_layer, images))\n",
    "#                     return {saliency.base.INPUT_OUTPUT_GRADIENTS: gradients}\n",
    "#             guided_ig_mask_3d = guided_ig.GetMask(image, call_model_function, call_model_args, x_steps=100, x_baseline=baseline, max_dist=1.0, fraction=0.5)\n",
    "#             outputs.append(guided_ig_mask_3d)    \n",
    "            \n",
    "#         if model_type =='cnn_based':\n",
    "#             cnn_classifier = temp_model_build(model) \n",
    "            \n",
    "#             def call_model_function(images, call_model_args=None, expected_keys=None):\n",
    "#                 target_class_idx =  call_model_args['class_idx_str']\n",
    "#                 images = tf.convert_to_tensor(images)\n",
    "#                 with tf.GradientTape() as tape:\n",
    "#                     tape.watch(images)\n",
    "#                     output_layer = cnn_classifier(images)\n",
    "#                     output_layer = output_layer[:,target_class_idx]\n",
    "#                     gradients = np.array(tape.gradient(output_layer, images))\n",
    "#                     return {saliency.base.INPUT_OUTPUT_GRADIENTS: gradients}       \n",
    "#             guided_ig_mask_3d = guided_ig.GetMask(image, call_model_function, call_model_args, x_steps=100, x_baseline=baseline, max_dist=1.0, fraction=0.5)\n",
    "#             outputs.append(guided_ig_mask_3d)               \n",
    "\n",
    "#     return np.array([abs(contribution.flatten()) for contribution in outputs])\n",
    "\n",
    "\n",
    "# def get_samples_classes(targets, categories):\n",
    "#     outputs = {}\n",
    "#     for category in categories:\n",
    "#         output = [target for target in targets if target[1] == category]\n",
    "#         images, labels = [], []\n",
    "#         for n in range(len(output)):\n",
    "#             images.append(output[n][0])\n",
    "#             labels.append(output[n][1])\n",
    "#         output = {'images':images,\n",
    "#                   'labels':labels}\n",
    "#         outputs[category] = output\n",
    "#     return outputs\n",
    "\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# label_ratios = [0.01, 0.1, 0.5, 1]\n",
    "# datasets = ['sat', 'lcz', 'eurosat', 'patternet']\n",
    "# random_seeds = [15, 16, 17, 18, 19]\n",
    "\n",
    "\n",
    "# ig_dict = {}\n",
    "# for dataset in datasets:\n",
    "#     ratio_ig_dict = {}\n",
    "#     for label_ratio in label_ratios:\n",
    "#         data = DataLoader(dataset, label_ratio)\n",
    "#         unlabeled_train_x, unlabeled_train_y, labeled_train_x, labeled_train_y, valid_x, valid_y, test_x, test_y = data.get_data()\n",
    "#         class_name = data.get_categories()\n",
    "\n",
    "#         if label_ratio == label_ratios[0]:\n",
    "#             vis_samples(test_x[:5])\n",
    "\n",
    "#         model_ig_dict = {}\n",
    "#         for model_type in ['qcnn', 'cnn_based']:\n",
    "#             obtained_model, report = get_model(saved_models, model_type, dataset, label_ratio)\n",
    "            \n",
    "#             pred_test_y, _ = obtained_model.predict(test_x)\n",
    "#             classification_matrix = confusion_matrix(np.argmax(test_y, axis=1), np.argmax(pred_test_y, axis=1))            \n",
    "#             temp = {}\n",
    "#             for seed in random_seeds:\n",
    "#                 set_seed(seed)\n",
    "#                 flatten_ig = get_guided_ig(obtained_model, model_type, seed)\n",
    "#                 targets = list(zip(flatten_ig, [class_name[int(label)] for label in np.argmax(test_y, axis=1)]))\n",
    "#                 flatten_contributions_class = get_samples_classes(targets, class_name)\n",
    "                \n",
    "#                 temp[seed] =  {'flatten_contributions_class': flatten_contributions_class}\n",
    "\n",
    "#             model_ig_dict[model_type] = {'multipleBasedlineResults': temp,\n",
    "#                                          'report': report,\n",
    "#                                          'confusion_matrix': classification_matrix}\n",
    "#         ratio_ig_dict[label_ratio] = model_ig_dict\n",
    "        \n",
    "#     ig_dict[dataset] = {'info': ratio_ig_dict, 'categories': class_name}\n",
    "    \n",
    "# with open('multiUniform_dev5_ig_dict_confusion_matrix.pkl', 'wb') as f:\n",
    "#     pickle.dump(ig_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "631b74d9-0256-46e2-8051-aa880267c399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model\n",
    "# from dataLoader import DataLoader\n",
    "\n",
    "\n",
    "# def get_data(dataset, label_ratio):\n",
    "#     data = DataLoader(dataset, label_ratio)\n",
    "#     unlabeled_train_x, unlabeled_train_y, labeled_train_x, labeled_train_y, valid_x, valid_y, test_x, test_y = data.get_data()\n",
    "#     class_name = data.get_categories()\n",
    "\n",
    "#     return labeled_train_x, labeled_train_y, test_x, test_y, class_name\n",
    "    \n",
    "# def get_model(dataset, label_ratio, saved_models, model_type):\n",
    "#     target_run = saved_models[model_type][dataset][str(label_ratio)]\n",
    "#     wandb_path = saved_models[model_type]['location']\n",
    "#     qcnn_model, test_acc = model.build_model(model_type, dataset, label_ratio, test_x.shape[1:], len(class_name), target_run, wandb_path)\n",
    "#     return qcnn_model, test_acc\n",
    "\n",
    "# def get_report(qcnn_model, test_x, test_y):\n",
    "#     y_pred, _  = qcnn_model.predict(test_x)\n",
    "#     y_pred = np.argmax(y_pred, axis=1)\n",
    "#     y_true = np.argmax(test_y, axis=1)\n",
    "#     class_report = classification_report(y_true ,y_pred, target_names=class_name, output_dict=True)\n",
    "#     return class_report\n",
    "\n",
    "# def get_dense_input(model, test_x):\n",
    "#     encoder = tf.keras.models.Model(inputs=[model.input], outputs=[model.get_layer('pqc').output])\n",
    "#     outputs = encoder.predict(test_x)\n",
    "#     return outputs   \n",
    "\n",
    "# def get_encoding_input(model, test_x):\n",
    "#     encoder = tf.keras.models.Model(inputs=[model.input], outputs=[model.get_layer('encoder').output])\n",
    "#     outputs = encoder.predict(test_x)\n",
    "#     return outputs\n",
    "\n",
    "\n",
    "# label_ratios = [0.01, 0.1, 0.5, 1]\n",
    "# datasets = ['sat', 'lcz', 'eurosat', 'patternet']\n",
    "\n",
    "\n",
    "# encoder_output = {}\n",
    "# for dataset in datasets:\n",
    "#     encoder_ratio = {}\n",
    "#     for label_ratio in label_ratios:\n",
    "#         train_x, train_y, test_x, test_y, class_name = get_data(dataset, label_ratio)\n",
    "#         encoder_model = {}\n",
    "#         for model_type in ['qcnn', 'cnn_based']:\n",
    "#             obtained_model, test_acc = get_model(dataset, label_ratio, saved_models, model_type)\n",
    "#             assert model.matrices(obtained_model, test_x, test_y, 'test')[3] == test_acc    \n",
    "            \n",
    "#             train_encoding = get_encoding_input(obtained_model, train_x)\n",
    "#             test_encoding = get_encoding_input(obtained_model, test_x)\n",
    "            \n",
    "#             train_pqc = get_dense_input(obtained_model, train_x)\n",
    "#             test_pqc = get_dense_input(obtained_model, test_x)\n",
    "            \n",
    "#             train_report = get_report(obtained_model, train_x, train_y)\n",
    "#             test_report = get_report(obtained_model, test_x, test_y)\n",
    "            \n",
    "#             train_label = [class_name[int(label)] for label in np.argmax(train_y, axis=1)]\n",
    "#             test_label = [class_name[int(label)] for label in np.argmax(test_y, axis=1)]\n",
    "            \n",
    "#             train_part = {'encoding': train_encoding,\n",
    "#                           'pqc': train_pqc,\n",
    "#                           'report': train_report,\n",
    "#                           'label': train_label}\n",
    "            \n",
    "#             test_part = {'encoding': test_encoding,\n",
    "#                          'pqc': test_pqc,\n",
    "#                          'report': test_report,\n",
    "#                          'label': test_label}            \n",
    "            \n",
    "            \n",
    "#             encoder_model[model_type] = {'train': train_part, 'test': test_part}                       \n",
    "#         encoder_ratio[label_ratio] = encoder_model\n",
    "#     encoder_output[dataset] = encoder_ratio\n",
    "       \n",
    "# import pickle \n",
    "\n",
    "# with open('model_output.pkl', 'wb') as f:\n",
    "#     pickle.dump(encoder_output, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9235c945-0185-498c-83d1-0fe6f5140a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def randomforestFeatureImportance(features, target_y, class_name):\n",
    "#     num_feature = features.shape[1]\n",
    "#     model = RandomForestClassifier()\n",
    "#     model.fit(features, target_y)\n",
    "    \n",
    "#     feature_importances = model.feature_importances_\n",
    "#     feature_names = [i for i in range(int(num_feature/2))] + [i for i in range(int(num_feature/2))]\n",
    "#     feature_source = ['qcnn']*int(num_feature/2) + ['cnn_based']*int(num_feature/2) \n",
    "\n",
    "#     importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances, 'Sources': feature_source})\n",
    "#     # sorted_df = importance_df.sort_values(by=['Importance'], ascending=False)\n",
    "#     return importance_df\n",
    "\n",
    "# label_ratios = [0.01, 0.1, 0.5, 1]\n",
    "# datasets = ['sat', 'lcz', 'eurosat', 'patternet']\n",
    "\n",
    "# featureImportancee_pqc_dict = {}\n",
    "\n",
    "# for dataset in datasets:\n",
    "#     featureImportancee_pqc_ = {}\n",
    "#     for label_ratio in label_ratios:\n",
    "#         data = DataLoader(dataset, label_ratio)\n",
    "#         unlabeled_train_x, unlabeled_train_y, labeled_train_x, labeled_train_y, valid_x, valid_y, test_x, test_y = data.get_data()\n",
    "#         class_name = data.get_categories()\n",
    "\n",
    "#         if label_ratio == label_ratios[0]:\n",
    "#             vis_samples(test_x[:5])\n",
    "\n",
    "#         qcnn_model, test_acc = get_model('qcnn', label_ratio, dataset, labeled_train_x.shape[1:], class_name)\n",
    "#         test = model.matrices(qcnn_model, test_x, test_y, 'test')\n",
    "#         assert test_acc == test[3]\n",
    "\n",
    "#         cnn_model, test_acc = get_model('cnn_based', label_ratio, dataset, labeled_train_x.shape[1:], class_name)\n",
    "#         test = model.matrices(cnn_model, test_x, test_y, 'test')\n",
    "#         assert test_acc == test[3]\n",
    "\n",
    "#         qcnn_extracted_pqc_features = get_dense_input(qcnn_model, test_x)\n",
    "#         cnn_extracted_pqc_features = get_dense_input(cnn_model, test_x)\n",
    "        \n",
    "#         for i in range(test_x.shape[0]):\n",
    "#             qcnn_extracted_pqc_features[i] = (qcnn_extracted_pqc_features[i]-np.min(qcnn_extracted_pqc_features[i]))/(np.max(qcnn_extracted_pqc_features[i])- np.min(qcnn_extracted_pqc_features[i]))\n",
    "#             cnn_extracted_pqc_features[i] = (cnn_extracted_pqc_features[i]-np.min(cnn_extracted_pqc_features[i]))/(np.max(cnn_extracted_pqc_features[i])- np.min(cnn_extracted_pqc_features[i]))\n",
    "            \n",
    "        \n",
    "#         print(np.max(qcnn_extracted_pqc_features), np.min(qcnn_extracted_pqc_features),np.max(cnn_extracted_pqc_features), np.min(cnn_extracted_pqc_features))\n",
    "        \n",
    "        \n",
    "\n",
    "#         extracted_pqc_features = np.concatenate((qcnn_extracted_pqc_features, cnn_extracted_pqc_features), axis=-1)\n",
    "\n",
    "#         target_y = [class_name[int(label)] for label in np.argmax(test_y, axis=1)]\n",
    "#         featureImportancee_pqc = randomforestFeatureImportance(extracted_pqc_features, target_y, class_name)\n",
    "#         featureImportancee_pqc_[label_ratio] = featureImportancee_pqc\n",
    "#     featureImportancee_pqc_dict[dataset] = featureImportancee_pqc_\n",
    "\n",
    "# import pickle \n",
    "\n",
    "# with open('combine_feature_importance_dict.pkl', 'wb') as f:\n",
    "#     pickle.dump(featureImportancee_pqc_dict, f) \n",
    "    \n",
    "    \n",
    "\n",
    "# # model_sep\n",
    "# def randomforestFeatureImportance(features, target_y, class_name, source):\n",
    "#     num_feature = features.shape[1]\n",
    "#     model = RandomForestClassifier()\n",
    "#     model.fit(features, target_y)\n",
    "    \n",
    "#     feature_importances = model.feature_importances_\n",
    "#     feature_names = [i for i in range(num_feature)]\n",
    "#     feature_source = [source]*num_feature\n",
    "\n",
    "#     importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances, 'Sources': feature_source})\n",
    "#     return importance_df\n",
    "\n",
    "# label_ratios = [0.01, 0.1, 0.5, 1]\n",
    "# datasets = ['sat', 'lcz', 'eurosat', 'patternet']\n",
    "# featureImportancee_pqc_dict = {}\n",
    "\n",
    "# for dataset in datasets:\n",
    "#     featureImportancee_pqc_ = {}\n",
    "#     for label_ratio in label_ratios:\n",
    "#         data = DataLoader(dataset, label_ratio)\n",
    "#         unlabeled_train_x, unlabeled_train_y, labeled_train_x, labeled_train_y, valid_x, valid_y, test_x, test_y = data.get_data()\n",
    "#         class_name = data.get_categories()\n",
    "\n",
    "#         if label_ratio == label_ratios[0]:\n",
    "#             vis_samples(test_x[:5])\n",
    "\n",
    "#         qcnn_model, test_acc = get_model('qcnn', label_ratio, dataset, labeled_train_x.shape[1:], class_name)\n",
    "#         test = model.matrices(qcnn_model, test_x, test_y, 'test')\n",
    "#         assert test_acc == test[3]\n",
    "\n",
    "#         cnn_model, test_acc = get_model('cnn_based', label_ratio, dataset, labeled_train_x.shape[1:], class_name)\n",
    "#         test = model.matrices(cnn_model, test_x, test_y, 'test')\n",
    "#         assert test_acc == test[3]\n",
    "\n",
    "#         qcnn_extracted_pqc_features = get_dense_input(qcnn_model, test_x)\n",
    "#         cnn_extracted_pqc_features = get_dense_input(cnn_model, test_x)\n",
    "        \n",
    "# #         for i in range(test_x.shape[0]):\n",
    "# #             qcnn_extracted_pqc_features[i] = (qcnn_extracted_pqc_features[i]-np.min(qcnn_extracted_pqc_features[i]))/(np.max(qcnn_extracted_pqc_features[i])- np.min(qcnn_extracted_pqc_features[i]))\n",
    "# #             cnn_extracted_pqc_features[i] = (cnn_extracted_pqc_features[i]-np.min(cnn_extracted_pqc_features[i]))/(np.max(cnn_extracted_pqc_features[i])- np.min(cnn_extracted_pqc_features[i]))\n",
    "            \n",
    "        \n",
    "#         print(np.max(qcnn_extracted_pqc_features), np.min(qcnn_extracted_pqc_features),np.max(cnn_extracted_pqc_features), np.min(cnn_extracted_pqc_features))\n",
    "        \n",
    "#         target_y = [class_name[int(label)] for label in np.argmax(test_y, axis=1)]\n",
    "\n",
    "#         qnn_featureImportancee_pqc = randomforestFeatureImportance(qcnn_extracted_pqc_features, target_y, class_name, 'qcnn')\n",
    "#         cnn_featureImportancee_pqc = randomforestFeatureImportance(cnn_extracted_pqc_features, target_y, class_name, 'cnn_based')\n",
    "\n",
    "\n",
    "#         featureImportancee_pqc_[label_ratio] = {'qcnn':qnn_featureImportancee_pqc, 'cnn_based':cnn_featureImportancee_pqc}\n",
    "#     featureImportancee_pqc_dict[dataset] = featureImportancee_pqc_\n",
    "\n",
    "# import pickle \n",
    "\n",
    "# with open('pqc_featureImportance_dict.pkl', 'wb') as f:\n",
    "#     pickle.dump(featureImportancee_pqc_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc6b88ae-cf74-46e6-9b48-5e4d60609009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('multiUniform_dev5_ig_dict_confusion_matrix.pkl', 'rb') as f:\n",
    "#     ig_dict = pickle.load(f)\n",
    "\n",
    "# def overall_ig(qcnn_flatten_contributions_class, class_name):\n",
    "#     temp_outputs = []\n",
    "#     for category in class_name:\n",
    "#         images = qcnn_flatten_contributions_class[category]['images']\n",
    "#         temp_outputs.append(images)\n",
    "#     temp_outputs = np.concatenate(temp_outputs)\n",
    "#     return temp_outputs\n",
    "\n",
    "# def avg_baseline_overallIG(ig_dict, model_type, dataset, ratio):\n",
    "#     imgs = []\n",
    "#     for i, key in enumerate(list(ig_dict[dataset]['info'][ratio][model_type]['multipleBasedlineResults'].keys())):\n",
    "#         img = ig_dict[dataset]['info'][ratio][model_type]['multipleBasedlineResults'][key]['flatten_contributions_class']\n",
    "#         categories = ig_dict[dataset]['categories']\n",
    "#         img = overall_ig(img, categories)\n",
    "#         imgs.append(img)\n",
    "#     imgs = np.array(imgs)\n",
    "#     return np.mean(imgs, axis=0)\n",
    "\n",
    "\n",
    "# def avg_baseline_class(ig_dict, model_type, dataset, ratio):\n",
    "#     outputs = {}\n",
    "#     for category in list(ig_dict[dataset]['info'][ratio][model_type]['multipleBasedlineResults'][15]['flatten_contributions_class'].keys()):\n",
    "#         imgs = []\n",
    "#         for i, key in enumerate(list(ig_dict[dataset]['info'][ratio][model_type]['multipleBasedlineResults'].keys())):\n",
    "#             img = ig_dict[dataset]['info'][ratio][model_type]['multipleBasedlineResults'][key]['flatten_contributions_class']\n",
    "#             target_img = img[category]['images']\n",
    "#             imgs.append(target_img)\n",
    "#         output = np.mean(imgs, axis=0)\n",
    "#         outputs[category] = output\n",
    "#     return outputs\n",
    "\n",
    "# def get_ig_model_cluster(ig_dict, model_type, dataset, label_ratio, groups):\n",
    "#     qnn_overall_ig = avg_baseline_overallIG(ig_dict, model_type, dataset, label_ratio)\n",
    "#     combined_ig = np.mean(qnn_overall_ig, axis=0)\n",
    "#     combined_ig = combined_ig.reshape(-1, 1)\n",
    "   \n",
    "#     kmeans = KMeans(n_clusters=groups)\n",
    "#     clusters = kmeans.fit_predict(combined_ig)\n",
    "    \n",
    "#     means = []\n",
    "#     for i in range(combined_ig.shape[0]):\n",
    "#         means.append([i, combined_ig[i], clusters[i]])\n",
    "    \n",
    "#     classes = []\n",
    "#     for i in range(groups):\n",
    "#         group = np.mean([element[1] for element in means if element[2]==i])\n",
    "#         classes.append([i, group])\n",
    "#     classes = sorted(classes, key=lambda x: (x[1]), reverse=True)\n",
    "        \n",
    "#     outputs = {}\n",
    "#     for i in range(groups):\n",
    "#         outputs[i] = [element[0] for element in means if element[2]==classes[i][0]]\n",
    "#     return outputs\n",
    "\n",
    "# def get_sep_feature_index(flatten_contributions_class, class_name):\n",
    "#     feature_index = []\n",
    "#     for n in range(64):\n",
    "#         minv_threshold = 0\n",
    "#         candidate = ''\n",
    "#         maxv_list = []\n",
    "\n",
    "#         for category in class_name:\n",
    "#             images = np.array(flatten_contributions_class[category])\n",
    "#             minv1, maxv1 = np.percentile(images[:,n], [25, 75])\n",
    "#             maxv_list.append([category, maxv1])\n",
    "#             if minv1 > minv_threshold:\n",
    "#                 minv_threshold = minv1\n",
    "#                 candidate = category\n",
    "                   \n",
    "#         if minv_threshold != 0:\n",
    "#             candidates = [element[1] for element in maxv_list if element[0]!=candidate]\n",
    "#             candidates = sorted(candidates, reverse=True)\n",
    "#             assert len(candidates) + 1 == len(class_name)\n",
    "#             if minv_threshold > candidates[0]:\n",
    "#                 feature_index.append([n, minv_threshold-candidates[0], candidate, minv_threshold])           \n",
    "#     return feature_index\n",
    "\n",
    "\n",
    "# def get_ig_level(ig_groups, feature_index, groups):\n",
    "#     results = []\n",
    "#     for group in range(groups):\n",
    "#         temp_group = ig_groups[group]\n",
    "#         if feature_index in temp_group:\n",
    "#             results.append(group)\n",
    "#     assert len(results)==1\n",
    "#     return results[0]\n",
    "\n",
    "\n",
    "# def get_sep_status(sep_features, feature_index):\n",
    "#     results = []\n",
    "    \n",
    "#     sep_feature_indice = [x[0] for x in sep_features]\n",
    "#     if feature_index in sep_feature_indice:\n",
    "#         results.append(True)\n",
    "        \n",
    "#     if len(results) == 0:\n",
    "#         return False\n",
    "#     else:\n",
    "#         assert len(results) == 1\n",
    "#         return results[0]     \n",
    "    \n",
    "# label_ratios = [0.01, 0.1, 0.5, 1]\n",
    "# datasets = ['sat', 'lcz', 'eurosat', 'patternet']\n",
    "# groups = 3\n",
    "\n",
    "# data = []\n",
    "# for dataset in datasets:\n",
    "#     class_name = ig_dict[dataset]['categories']\n",
    "#     for label_ratio in label_ratios:\n",
    "#         for model_type in ['qcnn', 'cnn_based']:\n",
    "#             ig_groups = get_ig_model_cluster(ig_dict, model_type, dataset, label_ratio, groups)\n",
    "            \n",
    "#             qnn_overall = avg_baseline_class(ig_dict, model_type, dataset, label_ratio)\n",
    "#             sep_features = get_sep_feature_index(qnn_overall, class_name)\n",
    "            \n",
    "#             for category in class_name:\n",
    "#                 qnn_overall_ig = qnn_overall[category]\n",
    "                \n",
    "#                 for i in range(qnn_overall_ig.shape[0]):\n",
    "#                     for j in range(qnn_overall_ig.shape[1]):\n",
    "#                         ig_level = get_ig_level(ig_groups, j, groups)\n",
    "#                         sep_status = get_sep_status(sep_features, j)\n",
    "#                         data.append([dataset, label_ratio, model_type, category, j, ig_level, sep_status, qnn_overall_ig[i,j]])\n",
    "\n",
    "\n",
    "# data_df = pd.DataFrame(data, columns=[\"dataset\", \"label_ratio\", \"model_type\", \"category\", \"feature\", \"level\", \"sep\", \"value\"])\n",
    "\n",
    "\n",
    "# output = {}\n",
    "# for dataset in datasets:\n",
    "#     output_ratio = {}\n",
    "#     output_ratio['categories'] = ig_dict[dataset]['categories']\n",
    "#     for label_ratio in label_ratios:\n",
    "#         output_model = {}\n",
    "#         for m, model_type in enumerate(['qcnn', 'cnn_based']): \n",
    "#             temp_df = data_df[(data_df['dataset']==dataset) & (data_df['label_ratio']==label_ratio)  & (data_df['model_type']==model_type)]\n",
    "#             important_features, sep_features, important_or_sep_features = [], [], [] \n",
    "#             important_unsep_features = []\n",
    "#             important_sep_features = []\n",
    "#             for i in tqdm(range(len(temp_df['value'].tolist()))):\n",
    "#                 value = temp_df['value'].tolist()[i]\n",
    "                    \n",
    "#                 if temp_df['level'].tolist()[i] != groups-1 or temp_df['sep'].tolist()[i]==True:\n",
    "#                     important_or_sep_features.append(value)\n",
    "#                 else:\n",
    "#                     important_or_sep_features.append(0)    \n",
    "                    \n",
    "#                 if temp_df['level'].tolist()[i] != groups-1:\n",
    "#                     important_features.append(value)\n",
    "#                 else:\n",
    "#                     important_features.append(0)    \n",
    "                \n",
    "#                 if temp_df['sep'].tolist()[i]:\n",
    "#                     sep_features.append(value)\n",
    "#                 else:\n",
    "#                     sep_features.append(0)    \n",
    "                    \n",
    "#                 if temp_df['level'].tolist()[i] != groups-1 and temp_df['sep'].tolist()[i]==False:\n",
    "#                     important_unsep_features.append(value)\n",
    "#                 else:\n",
    "#                     important_unsep_features.append(0)\n",
    "                    \n",
    "                    \n",
    "#                 if temp_df['level'].tolist()[i] != groups-1 and temp_df['sep'].tolist()[i]==True:\n",
    "#                     important_sep_features.append(value)\n",
    "#                 else:\n",
    "#                     important_sep_features.append(0)\n",
    "                    \n",
    "#             temp_df['importantUnsep_features'] = important_unsep_features\n",
    "#             temp_df['important_features'] = important_features\n",
    "#             temp_df['sep_features'] = sep_features\n",
    "#             temp_df['important_or_sep_features'] = important_or_sep_features\n",
    "#             temp_df['importantSep_features'] = important_sep_features\n",
    "    \n",
    "            \n",
    "#             output_model[model_type] = temp_df\n",
    "#         output_ratio[label_ratio] = output_model\n",
    "#     output[dataset] = output_ratio\n",
    "\n",
    "    \n",
    "# with open('encoder_dataframe_group3_all.pkl', 'wb') as f:\n",
    "#     pickle.dump(output, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f692c8cf-261f-4d5e-98f8-571dc3825031",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
